# Snap Detection V2 Config for ActionFormer
# Fixes: correct stride, wider segments, motion features
# Events: snap (0), qb_has_ball (1)

dataset_name: snap_v2
train_split: ['training']
val_split: ['validation']
devices: [0]
init_rand_seed: 42

dataset: {
  json_file: /opt/dlami/nvme/fcv-snap/actionformer/annotations/snap_v2.json,
  feat_folder: /opt/dlami/nvme/fcv-snap/actionformer/features_v2,
  file_prefix: ~,
  file_ext: .npy,
  num_classes: 2,  # snap, qb_has_ball
  input_dim: 1024,  # 512 RGB + 512 motion
  feat_stride: 4,   # 4 video frames between features
  num_frames: 16,   # 16 video frames per clip
  trunc_thresh: 0.5,
  crop_ratio: [0.9, 1.0],
  max_seq_len: 512,  # Longer sequences with finer stride
}

model: {
  fpn_type: identity,
  max_buffer_len_factor: 4.0,
  n_mha_win_size: 9,
  backbone_arch: [2, 2, 2],
  scale_factor: 2,
  # Regression ranges - must match backbone_arch length (3 levels)
  # With stride=4, segment of Â±5 frames = 2.5 feature steps
  regression_range: [[0, 4], [4, 8], [8, 16]],
}

opt: {
  learning_rate: 0.0001,
  epochs: 50,
  weight_decay: 0.05,
}

loader: {
  batch_size: 16,  # Larger batch for single GPU
  num_workers: 8,
}

train_cfg: {
  init_loss_norm: 100,
  clip_grad_l2norm: 1.0,
  cls_prior_prob: 0.01,
  center_sample: radius,
  center_sample_radius: 1.5,
}

test_cfg: {
  voting_thresh: 0.7,
  pre_nms_topk: 200,
  max_seg_num: 50,
  min_score: 0.001,
  multiclass_nms: True,
  nms_sigma: 0.5,
  # Relaxed IoU for event detection
  iou_threshold: 0.1,
}

output_folder: ./ckpt/snap_v2/
